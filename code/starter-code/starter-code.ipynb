{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "---\n",
    "title: MRJob lab\n",
    "type: lab\n",
    "duration: \"1:25\"\n",
    "creator:\n",
    "    name: Francesco Mosconi\n",
    "    city: SF\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) MRJob Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Introduction\n",
    "In the past lab we've used a Virtual Machine to run Map Reduce jobs on native hadoop. As you may have understood, it's quite cumbersome and complicated.\n",
    "\n",
    "Luckily we don't have to do that, because our friends at Yelp developed a great open source python library that wraps around hadoop streaming called [MRJob](https://github.com/Yelp/mrjob).\n",
    "\n",
    "This is already installed in your VM, but you can also install it locally if you prefer, using:\n",
    "\n",
    "`pip install mrjob`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## MRJob\n",
    "\n",
    "While Hadoop streaming is a simple way to do simple map-reduce tasks, it's complicated to use and not really friendly when things fail and we have to debug our code. Also, if we wanted to do a join from two different sets of data, it would be complicate to handle both with a single mapper, while it'd be much easier to have two separate mappers and one reducer.\n",
    "\n",
    "_MRJob_ is a library written and maintained by YELP that allows us to write map reduce jobs in python.\n",
    "\n",
    "Here's the word count map reduce, rewritten using MRJob."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "button": false,
    "collapsed": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "<module '__main__' (built-in)> is a built-in class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8874a46f0674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m      \u001b[0mMRWordFreqCount\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# load options from the command line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0mmr_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_READ_ARGS_FROM_SYS_ARGV\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0mmr_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mm\u001b[0m \u001b[0mmrjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mhelp\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMRJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmr_job_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/site-packages/mrjob/job.pyc\u001b[0m in \u001b[0;36mmr_job_script\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    676\u001b[0m         \"\"\"Path of this script. This returns the file containing\n\u001b[1;32m    677\u001b[0m         this class.\"\"\"\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;31m### Other useful utilities ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/inspect.pyc\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mno\u001b[0m \u001b[0mway\u001b[0m \u001b[0mcan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0midentified\u001b[0m \u001b[0mto\u001b[0m \u001b[0mget\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m     \"\"\"\n\u001b[0;32m--> 444\u001b[0;31m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.pyc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.pyo'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.py'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Madballa55/anaconda/lib/python2.7/inspect.pyc\u001b[0m in \u001b[0;36mgetfile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{!r} is a built-in class'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mismethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: <module '__main__' (built-in)> is a built-in class"
     ]
    }
   ],
   "source": [
    "\"\"\"The classic MapReduce job: count the frequency of words.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "     MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Notice that it's almost trivial: the mapper and the reducer become methods of our `MRWordFreqCount` class that inherits from the MRJob class.\n",
    "\n",
    "The script can be run in local mode with the following command:\n",
    "\n",
    "```bash\n",
    "python scripts/mrjob_wc.py data/project_gutenberg/1184-0.txt\n",
    "```\n",
    "\n",
    "Notice that it executes immediately on our VM. This is because if run in local mode MRJob will not use Hadoop to do the map-reduce. This will be very fast on small data, but become increasingly slow if data size increases.\n",
    "\n",
    "We can switch to hadoop mode by simply using the flag `-r hadoop` like this:\n",
    "\n",
    "```bash\n",
    "python scripts/mrjob_wc.py data/project_gutenberg/1184-0.txt -r hadoop\n",
    "```\n",
    "\n",
    "As you can see this wraps around hadoop streaming and it automates all the steps we did manually in the previous lecture including:\n",
    "\n",
    "- copying data to hdfs\n",
    "- running the data through hadoop streaming\n",
    "- copying back the output\n",
    "- removing temporary folders from hdfs\n",
    "\n",
    "Nice!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 1\n",
    "\n",
    "1. Use the code above to run the Word count on the whole project_gutenberg folder using MRJob\n",
    "1. Compare the execution time for the local mode and the hadoop mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {}
   },
   "outputs": [],
   "source": [
    "# Can't runon my computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 2: add a combiner\n",
    "\n",
    "\n",
    "A Combiner, also known as a semi-reducer, is an optional class that operates by accepting the inputs from the Map class and thereafter passing the output key-value pairs to the Reducer class.\n",
    "\n",
    "The main function of a Combiner is to summarize the map output records with the same key. The output (key-value collection) of the combiner will be sent over the network to the actual Reducer task as input.\n",
    "\n",
    "In MRJob these can be added simply by defining a method called `combiner`. Go ahead and modify the `MR` class adding a combiner. Then run it on the text files in the `datasets` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def reducer_find_max_word(self, _, word_count_pairs):\n",
    "    yield max(word_count_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 3: multi step jobs\n",
    "\n",
    "So far we've always worked with one step Map-reduce jobs. These are very simple. What if we wanted to perform a calculation that involves multiple steps? For example, what if we wanted to count the words in our documents and then find the most common word? This would involve the following steps:\n",
    "\n",
    "- map our text to a mapper that output pairs of (word, 1)\n",
    "- combine the pairs using the word as key (optional)\n",
    "- reduce the pairs using the word as key\n",
    "- find the word with the maximum count\n",
    "\n",
    "The last step can be achieved by chaining a new map reduce where the map function is the identity and the reduce function is something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "def reducer_find_max_word(self, _, word_count_pairs):\n",
    "    yield max(word_count_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Notice that we are aggregating over a blank key in order to get all possible word count pairs and then get the one with the maximum count.\n",
    "\n",
    "Go ahead and insert that into the MR class. In order to do that you'll need to use the `steps` function which is documented [here](https://pythonhosted.org/mrjob/job.html#mrjob.job.MRJob.steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "print \"Max for plato is the,195\"\n",
    "\n",
    "\"\"\"The classic MapReduce job: count the frequency of words.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer),\n",
    "            MRStep(reducer=self.reducer_find_max_word)\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield (word, sum(counts))\n",
    "\n",
    "    def reducer(self, word, counts):\n",
    "        yield None, (sum(counts), word)\n",
    "        \n",
    "    def reducer_find_max_word(self, _, word_count_pairs):\n",
    "        yield max(word_count_pairs)\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "     MRWordFreqCount.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 4: Setup and teardown of tasks\n",
    "\n",
    "MRJob allows you to write methods to run at the beginning and end of each mapper and reducer functions: the *_init() and *_final() methods:\n",
    "\n",
    "- mapper_init()\n",
    "- combiner_init()\n",
    "- reducer_init()\n",
    "- mapper_final()\n",
    "- combiner_final()\n",
    "- reducer_final()\n",
    "\n",
    "These functions are run only once at the beginning or at the end of each mapper and reducer and are useful for example when we want to have local variables available to the mapper.\n",
    "\n",
    "For example we could use `mapper_init` to load some kind of support file, like a sqlite3 database, or perhaps create a temporary fileor variable.\n",
    "\n",
    "Go ahead and rewrite the Word count using a `mapper_init` and `mapper_final` that do the following:\n",
    "\n",
    "- mapper_init should initialize a dictionary for the words\n",
    "- mapper should add the words to the dictionary as keys and increase the count each time the same word is ecountered\n",
    "- mapper_final should yield the (word, count) pairs contained in the dictionary\n",
    "- the reducer is going to be the same as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"The classic MapReduce job: count the frequency of words.\n",
    "\"\"\"\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MRWordFreqCount(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final,\n",
    " #                  combiner=self.combiner,\n",
    "                   reducer=self.combiner),\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.words_dict = {}\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            if not word.lower() in self.words_dict:\n",
    "                self.words_dict[word.lower()] = 0\n",
    "            self.words_dict[word.lower()] +=1\n",
    "                \n",
    "    def mapper_final(self):\n",
    "        for key in self.words_dict:\n",
    "            yield key, self.words_dict[key]\n",
    "\n",
    "    def combiner(self, word, counts):\n",
    "        yield word, sum(counts)\n",
    "\n",
    "#    def reducer(self, word, counts):\n",
    "#        yield None, (word, sum(counts))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "     MRWordFreqCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 5: Counters\n",
    "\n",
    "When we run a longer MR task, we may want to check the status of the calculation. This is achieved in MRJob using counters.\n",
    "\n",
    "Hadoop lets you track counters that are aggregated over a step. A counter has a group, a name, and an integer value. Hadoop itself tracks a few counters automatically. mrjob prints your jobâ€™s counters to the command line when your job finishes, and they are available to the runner object if you invoke it programmatically.\n",
    "\n",
    "To increment a counter from anywhere in your job, use the `increment_counter()` method.\n",
    "\n",
    "Go ahead and add a custom counter to the mapper function of the word count so that we know how many words it has processed. In order to see the counts, you may want to redirect the output of the job to a file or to `/dev/null` if you don't care about it.\n",
    "\n",
    "    python mrjob_counters.py assets/datasets > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 6: Putting it all together\n",
    "\n",
    "You've seen how powerful MRJob can be. Let's put it all together and write a class that returns the combined top 15 most frequent words in our text files. We can implement this in various ways, you're free to choose the one you think is more appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# Reducer\n",
    "\n",
    "def reducer_find_max_word(self, _, word_count_pairs):\n",
    "    word_count_pairs = list(word_count_pairs)\n",
    "    for x in range(15):\n",
    "        maximum = max(word_count_pairs)\n",
    "        word_count_pairs.remove(max(word_count_pairs))\n",
    "        yield maximum\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Exercise 7: Use Machine Learning to recognize a book\n",
    "\n",
    "Let's see if we can recognize a book from the most common words.\n",
    "\n",
    "- Run a local MRJob word count on a each file in the assets/datasets folder. Save the results to indivdual files for each book.\n",
    "- Run a local MRJob word count on the assets/mystery_book.txt and save the results to another file\n",
    "- Open the results in a pandas dataframe\n",
    "- Add a feature to each book that defines the author as it's target\n",
    "- Use Machine learning to predict which author the new book falls under\n",
    "- Who wrote the mystery book?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# mystery_response.txt\n",
    "# plato_response.txt, shake_response.txt, jane_response.txt, poe_response.txt, mark_response.txt\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "allFiles = glob.glob(r'../../*.txt')\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_, index_col=0, names = [file_[6:(len(file_)-13)]])\n",
    "    list_.append(df)\n",
    "frame = pd.concat(list_, axis = 1).fillna(0).T\n",
    "\n",
    "frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Additional Resources\n",
    "\n",
    "- [MRJob Documentation](https://pythonhosted.org/mrjob/)\n",
    "- [MRJob Examples](https://github.com/Yelp/mrjob/tree/master/mrjob/examples)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
